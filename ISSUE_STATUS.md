# Issue Status Tracking

**Last Updated**: 2026-01-06  
**CubeCL Version**: v0.8.1 (Validated)

This document tracks the completion status of open issues relative to what has been merged into the `experimental` branch.

## Summary

**PR #10** merged foundational infrastructure into `experimental` branch (commit ff87fec):
- CPU reference implementations for all 4 core kernels
- Basic benchmarking infrastructure
- Memory estimation utilities
- Project documentation and skills

**Recent Progress (2026-01-06):**
- âœ… CubeCL v0.8.1 API research completed (see `docs/cubecl-context.md`, `docs/cubecl-guide.md`)
- âœ… Created `src/kernels/cubecl/` module structure with config, interop, kernel scaffolding
- âœ… Updated dependencies: `cubecl = "0.8.1"`, `cubecl-cuda = "0.8.1"`
- âœ… Implemented Candle â†” CubeCL tensor conversion utilities

**Hardware Targets:**
- Phase 1: GeForce RTX 5080 (primary development)
- Phase 2: GeForce RTX 3090 Ti (validation)

## Issue-by-Issue Status

### Issue #5: [Kernel] Fused Flash Attention (Single-Pass QÂ·K^TÂ·V) GPU Kernel Implementation
**Status:** In Progress (Phase 1 - Minimal Viable Kernel)
**Priority:** Highest

- âœ… CPU reference implementation exists (`src/kernels/attention.rs`)
- âœ… Benchmarking infrastructure exists (`benches/kernels.rs`)
- âœ… Memory estimation utilities exist (`src/memory.rs`)
- âœ… CubeCL v0.8.1 API research completed
- âœ… Module structure created (`src/kernels/cubecl/`)
- âœ… Candle â†” CubeCL interop implemented
- âœ… Kernel configuration implemented (`FlashAttentionConfig`)
- ğŸš§ CubeCL GPU kernel implementation IN PROGRESS
- âŒ Numerical equivalence tests NOT done
- âŒ GPU benchmarks NOT done
- âŒ VRAM profiling NOT done

**Branch:** `feature/flash-attention-cubecl`  
**Estimated Completion:** 1-3 weeks for Phase 1 (RTX 5080 target)

---

### Issue #4, #8: [Kernel] Fused RMSNorm (with optional bias) GPU Kernel Implementation
**Status:** Not started (foundational work in place)
**Note:** Issues #4 and #8 are duplicates
- âœ… CPU reference implementation exists (`src/kernels/rmsnorm.rs`)
- âœ… Benchmarking infrastructure exists
- âŒ CubeCL GPU kernel NOT implemented
- âŒ Optional bias support NOT implemented
- âŒ Numerical equivalence tests NOT done
- âŒ GPU benchmarks NOT done

**Next steps:** Implement after Flash Attention is complete

---

### Issue #7: [Kernel] Fused Rotary Position Embedding (RoPE) GPU Kernel Implementation
**Status:** Not started (foundational work in place)
- âœ… CPU reference implementation exists (`src/kernels/rope.rs`)
- âœ… Benchmarking infrastructure exists
- âŒ CubeCL GPU kernel NOT implemented
- âŒ Fusion with Q/K computation NOT done
- âŒ Numerical equivalence tests NOT done
- âŒ GPU benchmarks NOT done

**Next steps:** Implement after Flash Attention is complete

---

### Issue #6: [Kernel] Fused SwiGLU Activation GPU Kernel Implementation
**Status:** Not started (foundational work in place)
- âœ… CPU reference implementation exists (`src/kernels/swiglu.rs`)
- âœ… Benchmarking infrastructure exists
- âŒ CubeCL GPU kernel NOT implemented
- âŒ Gate/up/down projection fusion NOT done
- âŒ Numerical equivalence tests NOT done
- âŒ GPU benchmarks NOT done

**Next steps:** Implement after Flash Attention is complete

---

### Issue #2: Comprehensive Kernel Benchmarking Suite for Performance & VRAM Profiling
**Status:** Partially complete (basic infrastructure only)
- âœ… Basic benchmarking infrastructure (`benches/kernels.rs`)
- âœ… CPU benchmarks for all 4 kernels
- âœ… Memory estimation utilities
- âŒ GPU benchmarking with CUDA profiling NOT done
- âŒ VRAM profiling across hardware configs NOT done
- âŒ CubeCL kernel benchmarks NOT possible (kernels don't exist yet)
- âŒ CI/CD integration NOT done
- âŒ Results documentation (`BENCHMARKS.md`) NOT created

**Dependencies:** Blocked on kernel implementations (#5, #6, #7, #8)

---

### Issue #1, #3: [Infra] Memory Pool Utility for Efficient VRAM Allocation
**Status:** Partially complete (basic structure only)
**Note:** Issues #1 and #3 are duplicates
- âœ… Basic `MemoryPool` struct exists (`src/memory.rs`)
- âœ… Allocation tracking implemented
- âœ… Memory estimation utilities implemented
- âŒ CubeCL integration NOT done
- âŒ Pre-allocation strategies NOT implemented
- âŒ Integration with fused kernels NOT done
- âŒ Benchmark validation NOT done

**Next steps:** Implement after core kernels are complete

---

### Issue #9: [Infra] CI/CD Branch Management & Merge Integration for Kernel Pipeline
**Status:** Not started
- âŒ Feature branches NOT created
- âŒ Merge automation NOT configured
- âŒ CI/CD pipeline NOT established
- âŒ Documentation NOT written

**Next steps:** Should be implemented in parallel with kernel work

---

## Current Priority: Flash Attention (Issue #5)

Flash Attention is the highest priority task because:
1. Marked as "Phase 1" milestone
2. Attention is the computational bottleneck in transformers
3. Targets 2-5x speedup and 70-80% VRAM reduction
4. Establishes the CubeCL kernel implementation pattern for subsequent work

**Active branch:** `feature/flash-attention-cubecl`

**Implementation Phases (Revised 2026-01-06):**

| Phase | Description | Hardware Target | Est. Time |
|-------|-------------|-----------------|----------|
| 1 | Minimal Viable Kernel | RTX 5080 | 1-3 weeks |
| 2 | Cross-GPU Validation | RTX 3090 Ti | 1-2 weeks |
| 3 | Advanced Features (f16, GQA) | Both | 2-4 weeks |
| 4 | Testing & Validation | Both | 1-2 weeks |
| 5 | Benchmarking | Both | 1-2 weeks |

**Current Phase 1 Progress:**
- [x] CubeCL API research (validated v0.8.1)
- [x] Module structure (`src/kernels/cubecl/`)
- [x] Tensor interop utilities
- [x] Kernel configuration
- [ ] Actual kernel implementation
- [ ] Test suite
- [ ] RTX 5080 profiling

**Reference Documents:**
- `docs/cubecl-context.md` - CubeCL v0.8.1 API reference
- `docs/cubecl-guide.md` - Implementation roadmap
- `CUBECL_IMPLEMENTATION_GUIDE.md` - Detailed kernel design
- `FLASH_ATTENTION_PLAN.md` - Phase breakdown
